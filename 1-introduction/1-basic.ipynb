{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Preliminary steps: \n",
    "1. Create a Docker container that runs ollama with an LLM model (e.g. DeepSeek). Pay attention to map the port to the contianer to a port of the localhost in order to access to ollama server inside the container from outside.\n",
    "2. Before runnign the script be sure that the container is up.\n",
    "\n",
    "\n",
    "Tutorial code adapted from [here](https://github.com/daveebbelaar/ai-cookbook/blob/main/patterns/workflows/1-introduction/2-structured.py).\n",
    "\n",
    "ollama supported model from [here](https://ollama.com/library/llama3)\n",
    "\n",
    "**Roles:**\n",
    "specifies who is speaking in the message history.\n",
    "| Role       | Description                 | Purpose                                      |\n",
    "|-----------|-----------------------------|----------------------------------------------|\n",
    "| **system**   | The system (developer-defined) | Provides instructions on AI behavior         |\n",
    "| **user**     | The human user             | Represents user queries or requests         |\n",
    "| **assistant** | The AI model               | Stores AI-generated responses               |\n",
    "| **tool**     | A function/tool that was called | Stores the output from a tool function |\n",
    "\n",
    "**Conversation history**:\n",
    "refers to the sequence of messages exchanged between the user and the AI model during a chat session. It includes all previous inputs (from the user) and outputs (from the model), as well as system instructions and function/tool responses.\n",
    "- Why is Conversation History Important?\n",
    "    - AI models are stateless, meaning they don’t \"remember\" past interactions unless the conversation history is passed back with each request.\n",
    "    - When using external tools (like get_weather), the model needs to track:\n",
    "        - The user’s original request.\n",
    "        - The function call response (e.g., weather data).\n",
    "        - The AI’s final answer based on the function output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first verse of \"Die Forelle\" (The Trout) by Franz Schubert:\n",
      "\n",
      "\"In die Tiefe sanken, fallen\n",
      "Schwangen sich wieder auf und Fallen\n",
      "Sank zum Grund des Tals\"\n",
      "(\"Deep into the depths I sink,\n",
      "Swirling back up and then sink,\n",
      "Down to the bottom of the valley\")\n",
      "\n",
      "Please let me know if you'd like more assistance!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1', #url map to or localhost where the ollama server inside the container is listening\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create( \n",
    "    # sends a request to the chat completions endpoint of your API server (in this case, running on http://localhost:11434/v1, indeed ollama is like a web server)\n",
    "    model=\"llama3\", # which model to use, attention: ollama model should be downloaded #deepseek-r1:8b\n",
    "    messages=[\n",
    "        {\n",
    "            #sets the behavior and context for the model.\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You're a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            # actual query \n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write the first verse of the lied 'die forelle' by Franz Schubert.\",\n",
    "        },\n",
    "    ],\n",
    "    #tools\n",
    "    #max_tokes\n",
    "    #response_format\n",
    "    #stram\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "    #choice[0] --> beacuse many answers can be returned\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The song \"Die Forelle\" (Op. 32) is set to a poem by Johann Mayrhofer, and the first verse goes like this:\n",
      "\n",
      "\"Ich fisch' den Fischen nach\"\n",
      "\n",
      "Translation:\n",
      "\"I am fishing after fish\"\n",
      "\n",
      "Let me know if you'd like me to continue!"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',  # Ollama server URL\n",
    "    api_key='ollama',  # Required but unused\n",
    ")\n",
    "\n",
    "# Send request with streaming enabled\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write the first verse of the lied 'Die Forelle' by Franz Schubert.\"},\n",
    "    ],\n",
    "    stream=True,  # Enable streaming\n",
    ")\n",
    "\n",
    "# Process the streamed response in real-time\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
